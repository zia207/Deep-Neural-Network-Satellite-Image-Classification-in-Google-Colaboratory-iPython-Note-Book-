{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8q9i7AdQv/p0mjmW1Zywk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/Deep-Neural-Network-Satellite-Image-Classification-in-Google-Colaboratory-iPython-Note-Book-/blob/master/NoteBook/Machine_Learning/Tree_based/03-01-01-00-tree-based-models-decision-tree-introduction-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1xLlN9eEG2IYFBlAuwl53aDVxcBkRnkEw\n",
        ")"
      ],
      "metadata": {
        "id": "AH9ES7RYWUdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Decision Trees\n",
        "\n",
        "Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by splitting the data into subsets based on the value of input features, creating a tree-like structure of decisions. The goal is to create a model that predicts the target variable by learning simple decision rules inferred from the data features. Decision trees are intuitive and easy to interpret, making them a popular choice for many machine learning tasks. By understanding the mathematical foundation of decision trees, including splitting criteria like Gini impurity, entropy, information gain, and mean squared error, we can build more effective and accurate models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TrQshXqjWS7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of tutorial is divided into several parts, each focusing on a specific type of decision tree model. The models covered include:\n",
        "\n",
        "1.1 [CART (Classification and Regression Trees)](https://github.com/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-01-tree-based-models-decision-tree-cart-python.ipynb)\n",
        "\n",
        "1.2 [Conditional Inference Trees (CITs)](https://github.com/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-02-tree-based-models-decision-tree-cit-python.ipynb)\n",
        "\n",
        "1.3 [Model-based Recursive Partitioning (MOB)](https://github.com/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-03-tree-based-models-decision-tree-mob-python.ipynb)\n",
        "\n",
        "1.4 [C4.5 Model](https://github.com/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-04-tree-based-models-decision-tree-c45-python.ipynb)\n",
        "\n",
        "1.5 [C5.0 Model](https://github.com/zia207/python-colab/blob/main/NoteBook/Machine_Learning/Tree_based/03-01-01-05-tree-based-models-decision-tree-c50-python.ipynb)\n"
      ],
      "metadata": {
        "id": "lBb_JilfrQkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of Decision Trees\n",
        "\n",
        "A **decision tree** is a supervised machine learning model used for both **classification** and **regression** tasks. It represents decisions and their possible consequences, including chance event outcomes, costs, and utility, in a tree-like structure. The model recursively splits the input space into regions based on feature values and makes a decision based on the majority class (classification) or average value (regression) in that region. Decision trees are popular due to their interpretability, ease of use, and ability to handle both numerical and categorical data."
      ],
      "metadata": {
        "id": "UHx3FkVeWnxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of a Decision Tree\n",
        "\n",
        "-   `Root Node`: The topmost node, representing the entire dataset.\n",
        "-   `Internal Nodes`: Represent decision points based on a feature and a threshold (e.g., `Petal.Length <= 2.5`).\n",
        "-   `Branches`: Represent the outcome of a decision, leading to child nodes.\n",
        "-   `Leaf Nodes`: Terminal nodes that provide the final prediction (e.g., a class label or a numerical value).\n",
        "-   `Splitting Criterion`: A measure to decide how to split a node (e.g., Gini impurity, variance reduction).\n"
      ],
      "metadata": {
        "id": "9TZggNGCrofU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Decision Trees Work\n",
        "\n",
        "1.  `Splitting`: Select a feature and threshold to split the data into two or more subsets, optimizing a criterion (e.g., reducing impurity).\n",
        "2.  `Recursion`: Repeat splitting for each subset until a stopping condition is met (e.g., maximum depth, minimum node size, or pure nodes).\n",
        "3.  `Prediction`: For a new observation, traverse the tree from the root to a leaf, following the decision rules, and return the leafâ€™s prediction."
      ],
      "metadata": {
        "id": "VYoXFAYgrqMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Decision Tree Models\n",
        "\n",
        "Decision tree models vary based on their splitting criteria, handling of data, and pruning strategies. Below are the main types of decision tree models, including their descriptions, mathematical foundations, and common implementations."
      ],
      "metadata": {
        "id": "9yij_QOcsSFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CART (Classification and Regression Trees)\n",
        "\n",
        "The most widely used decision tree algorithm, developed by Breiman et al. (1984). It supports both classification and regression and uses impurity-based splitting criteria.\n",
        "\n",
        "-   `Splitting Criteria`:\n",
        "\n",
        "  -   **Classification**: Gini impurity, $G = 1 - \\sum_{k=1}^K p_k^2$, where $p_k$ is the proportion of class $k$. Alternative: Entropy, $H = -\\sum_{k=1}^K p_k \\log(p_k)$.\n",
        "\n",
        "  -   **Regression**: Variance reduction, $\\text{Var} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2$, or Mean Squared Error (MSE).\n",
        "\n",
        "-   `Pruning`: Uses cost-complexity pruning, balancing tree size and error: $R_\\alpha(T) = R(T) + \\alpha \\cdot |T|$, where $R(T)$ is the error, $|T|$ is the number of nodes, and $\\alpha$ is the complexity parameter (`cp`).\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "  -   Binary splits (two child nodes per split).\n",
        "\n",
        "  -   Prone to bias toward features with many levels.\n",
        "\n",
        "  -   Simple and fast but may overfit without pruning."
      ],
      "metadata": {
        "id": "BMTkykugsZXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional Inference Trees (CITs)\n",
        "\n",
        "Developed by Hothorn et al. (2006), CITs use a statistical framework to reduce bias in variable selection and overfitting.\n",
        "\n",
        "-   `Splitting Criteria`: - Based on statistical hypothesis tests (e.g., permutation tests) to assess the association between a predictor $X_j$ and the response $Y$.\n",
        "\n",
        "   -   **Classification**: Chi-squared or permutation tests for categorical responses.\n",
        "\n",
        "   -   **Regression**: F-tests or correlation-based tests for continuous responses.\n",
        "\n",
        "-   `Test statistic` $T_j = \\sum_{i=1}^n g(X_{ij}) h(Y_i)$ with p-values adjusted (e.g., Bonferroni) to select the predictor with the smallest p-value.\n",
        "\n",
        "-   `Pruning`: Implicitly controlled by a significance threshold (`mincriterion`, typically 0.95, corresponding to $\\alpha = 0.05$.\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "   -   Unbiased variable selection, avoiding preference for features with many split points.\n",
        "\n",
        "    -   Statistically rigorous, reducing overfitting.\n",
        "\n",
        "    -   Binary splits, similar to CART."
      ],
      "metadata": {
        "id": "ALQW4gRwskbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C4.5\n",
        "\n",
        "C4.5 is a supervised learning algorithm that builds a decision tree by recursively splitting the input space into regions based on feature values and assigning a class label to each region.\n",
        "\n",
        "-   `Splitting Criterion`:\n",
        "\n",
        "  -   **Gain Ratio**: Normalizes information gain by the intrinsic information of the split, $\\text{Gain Ratio} = \\frac{\\text{Gain}}{-\\sum_{i} \\frac{n_i}{n} \\log(\\frac{n_i}{n})}$.\n",
        "\n",
        "-   `Pruning`: Uses error-based pruning, removing branches that do not improve accuracy on a validation set.\n",
        "\n",
        "-   `Characteristics`:\n",
        "\n",
        "  -   Supports continuous features with binary splits (e.g., `X <= t`).\n",
        "\n",
        "   -   Multi-way splits for categorical features.\n",
        "\n",
        "   -   More robust than ID3 but less common than CART or CITs in modern implementations."
      ],
      "metadata": {
        "id": "GcQ6e5NwsmIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model-based Recursive Partitioning (MOB)\n",
        "\n",
        "MOB combines decision trees with parametric models (e.g., linear regression, logistic regression) fitted in terminal nodes. Itâ€™s designed to capture heterogeneous relationships.\n",
        "\n",
        "-   `Splitting Criterion`: - Statistical tests (e.g., score tests) to check parameter stability across splits.\n",
        "\n",
        "-   `Test statistic` based on the score function $T_j = \\sum \\psi_i(Z_{ij})$, where $\\psi_i$ is the score of the parametric model.\n",
        "\n",
        "-   `Characteristics`: - Fits a full parametric model (e.g., ( Y = \\beta\\_0 + \\beta\\_1 X_1 )) in each leaf. - Unbiased splitting, similar to CITs. - Ideal for data with varying predictor effects across subgroups."
      ],
      "metadata": {
        "id": "JeF9ShfAsqcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison of Decision Tree Models\n",
        "\n",
        "| Model | Splitting Criterion | Task Support | Pruning | Bias Handling | Implementation |\n",
        "|------------|------------|------------|------------|------------|------------|\n",
        "| **CART** | Gini, Entropy (classification); Variance (regression) | Classification, Regression | Cost-complexity pruning | Biased toward many splits | `rpart`, `sklearn` |\n",
        "| **CIT** | Statistical tests (chi-squared, F-test) | Classification, Regression | Statistical stopping | Unbiased | `partykit::ctree` |\n",
        "| **MOB** | Parameter stability tests | Classification, Regression | Statistical stopping | Unbiased | `partykit::mob` |\n",
        "| **C4.5** | Gain Ratio | Classification | Error-based pruning | Less biased | `RWeka::J48` |\n",
        "| **C5.0** | Gain Ratio | Classification | Error-based pruning | Less biased | `C50:C5.0` |"
      ],
      "metadata": {
        "id": "X1rZrirXst9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Differences Between Classification and Regression\n",
        "\n",
        "| **Aspect** | **Classification** | **Regression** |\n",
        "|-------------------|---------------------------|--------------------------|\n",
        "| **Output** | Discrete class labels | Continuous numerical values |\n",
        "| **Splitting Criterion** | Gini, Entropy, Misclassification Error | Variance Reduction, MSE |\n",
        "| **Leaf Node Output** | Majority class | Mean (or median) of target values |\n",
        "| **Objective** | Maximize class purity | Minimize variance of target values |"
      ],
      "metadata": {
        "id": "d6kg0xFGRyj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "Decision trees are powerful and interpretable models for classification and regression tasks. They can be implemented using various algorithms, each with its strengths and weaknesses. The choice of algorithm depends on the specific problem, data characteristics, and desired interpretability. Understanding the differences between these models helps in selecting the most appropriate one for a given task."
      ],
      "metadata": {
        "id": "-V6D09SpR2--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refereences\n",
        "\n",
        "\n",
        "\n",
        "1.  Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1986). Classification and regression trees. Wadsworth and Brooks/Cole Advanced Books & Software.\n",
        "\n",
        "2.  Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction (Vol. 2). Springer Science & Business Media.\n",
        "\n",
        "3.  James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112). New York: Springer.\n",
        "\n",
        "4.  Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81-106."
      ],
      "metadata": {
        "id": "HZtIoviVSGOk"
      }
    }
  ]
}